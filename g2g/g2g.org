#+begin_export latex
\section{Introduction}
As described in Chapter~\ref{ch:cli}, gene2go files are used by
\ty{ego} to calculate the enrichment of GO terms.  Table~\ref{tab:g2g}
shows an example of such a gene2go table. It has eight columns, of
which four are significant, gene ID, GO ID, GO term, and category. The
insignificant columns are denoted by dots.
\begin{table}
  \caption{Example gene2go table.}\label{tab:g2g}
  \begin{center}
    \resizebox{\textwidth}{!}{
  \begin{tabular}{cccccccc}
    \hline
    Taxon ID & Gene ID & GO ID & Evidence & Qualifier & GO term &
    Pubmed & Category\\\hline
    . & 10006 & GO:0001501 & . & . & skeletal system development & . & Process\\
    . & 1000 &  GO:0001501 & . & . & skeletal system development & . & Process\\
    ...\\
    \hline
  \end{tabular}
  }
\end{center}
\end{table}
A gene2go file is constructed from three maps, genes to proteins,
proteins to GO IDs, and GO IDs to GO terms. In the following we
construct each of these maps for the human genome in turn before we
join them to construct the final gene2go file.
\section{Map Genes to Proteins}
To map genes to protein, we need the human GFF file, which we download
using \ty{datasets}.
#+end_export
#+begin_src sh <<g2g>>=
  datasets download genome accession GCF_000001405.40 \
	   --include gff3 --filename gff3.zip
#+end_src
#+begin_export latex
We unzip the downloaded data and move it to a convenient location.
#+end_export
#+begin_src sh <<g2g>>=
  unzip -d gff3 gff3.zip
  mv gff3/ncbi_dataset/data/GCF_000001405.40/genomic.gff \
     hs.gff
#+end_src
#+begin_export latex
For each CDS in the human genome, we extract the database
cross-reference annotation. Since keys like ``GenBank'' and
``Genbank'' are synonymous, we convert everything to upper case. From
that we pick the gene and protein IDs by calling \ty{pick.awk}, which
we still need to write.
#+end_export
#+begin_src sh <<g2g>>=
  awk '$3 == "CDS"' hs.gff |
      tr ';' '\n' |
      grep '^Dbxref' |
      sed 's/Dbxref=//' |
      tr ':,' ' ' |
      tr [:lower:] [:upper:] |
      awk -f pick.awk > gen2pro.txt
#+end_src
#+begin_export latex
In the program \ty{pick.awk}, we store the IDs of the genes and
proteins found in each set of database cross-references, and print
them at the end.
#+end_export
#+begin_src awk <<pick.awk>>=
  {
    ##<<Store genes and proteins, Ch. \ref{ch:g2g}>>
  }
  END {
    ##<<Print genes and proteins, Ch. \ref{ch:g2g}>>
  }
#+end_src
#+begin_export latex
Given a row of database cross-references, we walk along its tokens and
look for the all-caps keys ``GENEID'' for the gene ID and ``GENBANK''
for the protein ID. Then we store these two values in a hash map.
#+end_export
#+begin_src awk <<Store genes and proteins, Ch. \ref{ch:g2g}>>=
  for (i = 1; i <= NF; i += 2) {
    if ($i == "GENEID")
      g = $(i+1)
    if ($i == "GENBANK")
      p = $(i+1)
  }
  map[g] = p
#+end_src
#+begin_export latex
At the end we iterate across the gene IDs and print them together with
the corresponding protein ID.
#+end_export
#+begin_src awk <<Print genes and proteins, Ch. \ref{ch:g2g}>>=
  for (g in map)
    printf "%s\t%s\n", g, map[g]
#+end_src
#+begin_export latex
\section{Map Proteins to GO IDs}
Proteins are mapped to their GO IDs using homology searches. So we
begin by downloading the human proteome, again using our friend
\ty{datasets}.
#+end_export
#+begin_src sh <<g2g>>=
  datasets download genome accession GCF_000001405.40 \
	   --include protein --filename protein.zip
#+end_src
#+begin_export latex
We unzip the downloaded file and move the proteome to a convenient
place.
#+end_export
#+begin_src sh <<g2g>>=
  unzip -d protein protein.zip
  mv protein/ncbi_dataset/data/GCF_000001405.40/protein.faa hs.faa
#+end_src
#+begin_export latex
We use the eggnog software to map proteins to GO
IDs~\cite{hue19:egg,can21:egg}. To start with, we install the eggnog
software by following the instructions posted in their github
repository\footnote{\ty{github.com/eggnogdb/eggnog-mapper}}. This
seems to boil down to four steps, clone the software, change into its
directory, setup the system, and download the eggnog data. The last
two steps, system setup and data download, take some time and require
a bit of disk space, so be prepared.
#+end_export
#+begin_src sh <<g2g>>=
  git clone https://github.com/eggnogdb/eggnog-mapper
  cd eggnog-mapper/
  python setup.py install
  python download_eggnog_data.py
#+end_src
#+begin_export latex
We run the eggnog-mapper in ultra-sensitive mode and ask for
``non-electronic'' GO IDs, i. e. high-quality annotations. The
output is stored in the directory \ty{res}.
#+end_export
#+begin_src sh <<g2g>>=
  emapper.py --cpu 64 -o human --output_dir res/ \
	     --override -m diamond --sensmode ultra-sensitive \
	     --dmnd_ignore_warnings -i hs.faa \
	     --evalue 0.001 --score 60 --pident 40 \
	     --query_cover 20 --subject_cover 20 \
	     --itype proteins --tax_scope inner_narrowest \
	     --target_orthologs all \
	     --go_evidence non-electronic --dbmem \
	     --tax_scope eukaryota --pfam_realign none \
	     --report_orthologs --decorate_gff yes
#+end_src
#+begin_export latex
The directory \ty{res} now contains the file
\begin{verbatim}
human.emapper.annotations
\end{verbatim}
Column 1 of this file contains the protein ID, column 10 the
corresponding GO IDs. We convert this into pairs of protein IDs and GO
IDs.
#+end_export
#+begin_src sh <<g2g>>=
  grep -v '^#' human.emapper.annotations |
      cut -f 1,10 |
      tr ',' ' ' |
      awk '{for(i=2;i<=NF;i++)printf "%s\t%s\n", $1, $i}' \
	  > pro2go.txt
#+end_src
#+begin_export latex
\section{Map GO IDs to GO Terms}
GO terms give biological meaning to GO IDs, for example the GO ID
GO:0001501 refers to the term ``skeletal system development''. Apart
from its term, a GO ID is also mapped to a category, which we would
like to include in our map. Our example GO ID refers to the category
``Process''. A current list of GO IDs, terms, and categories are
distributed by the GO Consortium via their website
\ty{geneontology.org}. We download this list via the URL supplied by
the GO Consortium.
#+end_export
#+begin_src sh <<g2g>>=
  wget http://purl.obolibrary.org/obo/go/go-basic.obo
#+end_src
#+begin_export latex
We are interested in GO Terms, which start with
\begin{verbatim}
[Term]
\end{verbatim}
and end with an empty line. We extract the first one.
#+end_export
#+begin_src sh <<g2g>>=
  awk '/^\[Term\]/{o=1}/^$/{o=0}o{print}' \
      go-basic.obo | head
#+end_src
#+begin_export latex
\begin{verbatim}
[Term]
id: GO:0000001
name: mitochondrion inheritance
namespace: biological_process
def: "The distribution of mitochondria,...
synonym: "mitochondrial inheritance" EXACT []
is_a: GO:0048308 ! organelle inheritance
is_a: GO:0048311 ! mitochondrion distribution
[Term]
id: GO:0000002
\end{verbatim}
#+end_export
#+begin_export latex
At the time of writing there were 47,825 terms.
#+end_export
#+begin_src sh <<g2g>>=
  grep -c '^id: GO' go-basic.obo
#+end_src
#+begin_export latex
For each term we wish to extract the values of the three keys id,
name, and namespace. This would be simplified if we could rely on the
order of keys we observe in the first term. So we print the three keys
in a row by calling \ty{printKeys.awk} and count the number of
distinct rows. We find that there's only one, as hoped.
#+end_export
#+begin_src sh <<g2g>>=
  awk -f printKeys.awk go-basic.obo |
      sort |
      uniq
#+end_src
#+begin_export latex
In the program \ty{printKeys.awk} we print ensure we're inside a term
and print the keys.
\bpr{printKeys.awk}{pr:pk}
#+end_export
#+begin_src awk <<printKeys.awk>>=
  ##<<Ensure we're inside a term, Pr. \ref{pr:pk}>>
  ##<<Print keys, Pr. \ref{pr:pk}>>
#+end_src
#+begin_export latex
\epr A term starts with \verb+[Term]+. If we find this header, we
switch on the indicator variable \ty{term}, otherwise we switch it
off.
#+end_export
#+begin_src awk <<Ensure we're inside a term, Pr. \ref{pr:pk}>>=
  /^\[/ {
    if (/\[Term\]/)
      term = 1
    else
      term = 0
  }
#+end_src
#+begin_export latex
If we're inside a term, we parse for id, name, and namespace. The
first two we print without newline, the third with.
#+end_export
#+begin_src awk <<Print keys, Pr. \ref{pr:pk}>>=
  term && /^id:/ {
    printf "id"
  }
  term && /^name:/ {
    printf "name"
  }
  term && /^namespace: / {
    printf "namespace\n"
  }
#+end_src
#+begin_export latex
Given that we can rely on the order of keys, we print their values to
the file \ty{go2term.txt} by calling \ty{printVal.awk}.
#+end_export
#+begin_src sh <<g2g>>=
  awk -f printVal.awk go-basic.obo > go2term.txt
#+end_src
#+begin_export latex
The program \ty{printVal.awk} is structured like \ty{printKeys.awk}
and we print the columns of values separated by
tabs.\bpr{printVal.awk}{pr:pv}
#+end_export
#+begin_src awk <<printVal.awk>>=
  ##<<Ensure we're inside a term, Pr. \ref{pr:pk}>>
  ##<<Print Values, Pr. \ref{pr:pv}>>
#+end_src
#+begin_export latex
\epr The ``id'' and the ``namespace'' each have a single string as
value. However, the ``name'' usually consists of more than one string,
so we iterate over them.
#+end_export
#+begin_src awk <<Print Values, Pr. \ref{pr:pv}>>=
  term && /^id:/ {
    printf "%s", $2
  }
  term && /^name:/ {
    printf "\t%s", $2
    for (i = 3; i <= NF; i++)
      printf " %s", $i
  }
  term && /^namespace: / {
    printf "\t%s\n", $2
  }
#+end_src
#+begin_export latex
We count the number of rows in \ty{go2term.txt}, to find the expected
47,825.
#+end_export
#+begin_src sh <<g2g>>=
  wc -l go2term.txt
#+end_src
#+begin_export latex
\section{Join Tables}
We now have three tables in hand
\begin{enumerate}
\item \ty{gen2pro.txt}: gene IDs mapped to protein IDs
\item \ty{pro2go.txt}: protein IDs mapped to GO IDs
\item \ty{go2term.txt}: go IDs mapped to GO terms (and categories)
\end{enumerate}
Our aim is to construct the table shown in Table~\ref{tab:g2g}.
To get this, we begin by joining \ty{gen2pro.txt} and \ty{pro2go.txt}
on the protein ID. For this, we sort \ty{gen2pro.txt} and \ty{pro2go}
on the protein ID.
#+end_export
#+begin_src sh <<g2g>>=
  sort -k 2,2 gen2pro.txt > t; mv t gen2pro.txt
  sort -k 1,1 pro2go.txt > t; mv t pro2go.txt
#+end_src
#+begin_export latex
Now we join these tables, extract the gene IDs and GO terms, and save
the result in the file \ty{gen2go.txt}.
#+end_export
#+begin_src sh <<g2g>>=
  join -1 2 gen2pro.txt pro2go.txt |
      awk '{printf "%s\t%s\n", $2, $3}' > gen2go.txt
#+end_src
#+begin_export latex
Next, we join \ty{gen2go.txt} with \ty{go2term.txt} on the GO ID. So
we sort both files on that column.
#+end_export
#+begin_src sh <<g2g>>=
  sort -k 2,2 gen2go.txt > t; mv t gen2go.txt
  sort -k 1,1 go2term.txt > t; mv t go2term.txt
#+end_src
#+begin_export latex
Now we join these two tables and generate the desired gene2go table by
calling the script \ty{print.awk}, which we still need to write. Note
that here \ty{join} takes as delimiter (\ty{-t}) the TAB character,
which is entered in single quotes by pressing C-v TAB. We save the
final table in the file \ty{hs.g2g}.
#+end_export
#+begin_src sh <<g2g>>=
  join -t '      ' -1 2 gen2go.txt go2term.txt |
      awk -F '\t' -f print.awk  > hs.g2g
#+end_src
#+begin_export latex
In \ty{print.awk}, we generate the eight columns of
Table~\ref{tab:g2g}. We do this by first declaring a formatting
template, which is then used for every line parsed.
#+end_export
#+begin_src awk <<print.awk>>=
  BEGIN {
    tmpl = ".\t%s\t%s\t.\t.\t%s\t.\t%s\n"
  }
  {
    printf tmpl, $2, $1, $3, $4
  }
#+end_src
